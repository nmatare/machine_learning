---
title: "Machine Learning 41204/01 - Midterm"
author: "Nathan Matare"
date: "2/12/2017"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r config, include=FALSE}

	options("width" = 250)
	options(scipen  = 999)
	options(digits  = 003)

	library(ggplot2); require(gridExtra); library(MASS); library(Matrix); library(parallel)
	library(kknn); library(boot); library(rpart); library(data.table); library(foreach)
	library(doMC); library(doRNG)
	library(gamlr); library(BayesTree); library(xgboost); library(ranger)

	set.seed(666) # the devils seed

	username <- Sys.info()[["user"]]
	dir <- paste("/home/", username, "/projects/machine_learning/midterm/", sep = ""); setwd(dir)
	setwd(dir)

	# Helper functions from TA
	mse <- function(y,yhat) {return(sum((y - yhat) ^ 2))}

	doknn <- function(x, y, xp, k){
					kdo=k[1]
					train = data.frame(x,y=y)
					test = data.frame(xp); names(test) = names(train)[1:(ncol(train)-1)]
					near  = kknn(y~.,train,test,k=kdo,kernel='rectangular')
					return(near$fitted)
	}

	docv <- function(x, y, set, predfun, loss, nfold = 10, doran = TRUE, verbose = TRUE, ...){
					#a little error checking
					if(!(is.matrix(x) | is.data.frame(x))) {cat('error in docv: x is not a matrix or data frame\n'); return(0)}
					if(!(is.vector(y))) {cat('error in docv: y is not a vector\n'); return(0)}
					if(!(length(y)==nrow(x))) {cat('error in docv: length(y) != nrow(x)\n'); return(0)}

					nset = nrow(set); n=length(y) #get dimensions
					if(n==nfold) doran=FALSE #no need to shuffle if you are doing them all.
					cat('in docv: nset,n,nfold: ',nset,n,nfold,'\n')
					lossv = rep(0,nset) #return values
					if(doran) {ii = sample(1:n,n); y=y[ii]; x=x[ii,,drop=FALSE]} #shuffle rows

					fs = round(n/nfold) # fold size
					for(i in 1:nfold) { #fold loop
					bot=(i-1)*fs+1; top=ifelse(i==nfold,n,i*fs); ii =bot:top
					if(verbose) cat('on fold: ',i,', range: ',bot,':',top,'\n')
					xin = x[-ii,,drop=FALSE]; yin=y[-ii]; xout=x[ii,,drop=FALSE]; yout=y[ii]
						for(k in 1:nset) { #setting loop
						  yhat = predfun(xin,yin,xout,set[k,],...)
						  lossv[k]=lossv[k]+loss(yout,yhat)
						} 
					} 
	  				return(lossv)
	}

	docvknn <- function(x, y, k, nfold = 10, doran = TRUE, verbose = TRUE){return(docv(x, y, matrix(k, ncol = 1), doknn, mse, nfold = nfold, doran = doran, verbose = verbose))}

	lossMR <- function(y, phat, thr = 0.5){
					if(is.factor(y)) 
						y <- as.numeric(y) - 1
					yhat <- ifelse(phat > thr, 1, 0)
					return(1 - mean(yhat == y))
	}

	multiplot <- function(..., plotlist = NULL, file, cols=1, layout = NULL) {
	  require(grid)

	  # Make a list from the ... arguments and plotlist
	  plots <- c(list(...), plotlist)

	  numPlots = length(plots)

	  # If layout is NULL, then use 'cols' to determine layout
	  if (is.null(layout)) {
	    # Make the panel
	    # ncol: Number of columns of plots
	    # nrow: Number of rows needed, calculated from # of cols
	    layout <- matrix(seq(1, cols * ceiling(numPlots/cols)),
	                    ncol = cols, nrow = ceiling(numPlots/cols))
	  }

	 if (numPlots==1) {
	    print(plots[[1]])

	  } else {
	    # Set up the page
	    grid.newpage()
	    pushViewport(viewport(layout = grid.layout(nrow(layout), ncol(layout))))

	    # Make each plot, in the correct location
	    for (i in 1:numPlots) {
	      # Get the i,j matrix positions of the regions that contain this subplot
	      matchidx <- as.data.frame(which(layout == i, arr.ind = TRUE))

	      print(plots[[i]], vp = viewport(layout.pos.row = matchidx$row,
	                                      layout.pos.col = matchidx$col))
	    }
	  }# for plotting
	}

```

## Question 1

* Part 1:
    + A) TRUE - Increasing the number of nearest neighbors (k) will result in a more simple function; thus, as model complexity decreases, bias increases. See below Figure 1
    + B) FALSE - Increasing the number of nearest neighbors (k) will result in a more simple function; thus as model complexity decreases, variance decreases. See below Figure 1
    
     ![Bias vs Variance](/home/nmatare/projects/machine_learning/midterm/biasvsvar.png)
     
\newpage
  
    + C) TRUE - Consider a kNN at K = 1 on training data; the model will perfectly fit to its nearest neighbor(itself)--perfectly capturing the relationship; thus as k increases, the model becomes more simple/general and incorporates more neighbores. This results in an increasing misclassification rate on the training set. 
    
    + D) UNKNOWN - Dependent on the value of K, the misclassification rate could increase as K increases--the data favoring a more simple model. Or as K increases, the misclassification rate could decrease--the data favoring a more complex model. It depends on the specific value of K, the nature of the data, and whether or not the test data accurately mimics the training data. See Figure 2.

     ![RMSE vs Complexity](/home/nmatare/projects/machine_learning/midterm/complex-RMSE.png)
   
\newpage

* Part 2:
    + A) TRUE - Model 2 is more complex than Model 1; thus as model complexity increases, variance increases. See below Figure 3. 
    + B) TRUE -  Model 3 is more complex than Model 2; thus as model complexity increases, bias decreases. See below Figure 3.
    
    ![Bias vs Variance](/home/nmatare/projects/machine_learning/midterm/biasvsvar.png)
    
    + C) TRUE - Model 3 is overfit to the training data; however it has the smallest training error due to its low bias. 
    + D) FALSE - Model 1 is underfit to the data and does not capture the non-linear relationship well; Model 2 would ostensibly have the smallest test error provided that the testing data mirrors the training data
    
* Part 3: FALSE -  Never is a strong word. When training a model on a training dataset, the aforementioned model will usually fit better to all the data it currently has ‘seen’. Thus, provided the model is overfit, the model will subsequently produce greater error on the unseen validation dataset. However, it could be that the model actually evaluates better on the validation dataset (by chance, oddities in the data, or bad model parameters); this is why we perform multiple random folds of cross-validation. 

* Part 4: Slightly TRUE - leave-one-out cross validation is an approximate of an unbiased estimation of the true error; however for theoretical reasons, it is not exactly unbiased^[ http://ai.stanford.edu/~ronnyk/accEst.pdf]. Further, leave-one-out cross validation constrains the heterogeneity of the evaluated data and can produce underestimated predictions. The industry standard is to use 5 or 10 fold cross validation. 

\newpage

## Question 2

```{r question2_dataload, include = FALSE}
data <- as.data.table(read.csv("PhillyCrime.csv"))
```

* Part 1 - I observe a concentration of vandalism around coordinates [40.00, -75.15]; similarly, thefts appear to be more commonplace around the city's main middle region.

```{r question2_part1, include = TRUE, echo=FALSE, width = 40, height = 60}
    ggplot(data = data, aes(x = X, y = Y, color = Category)) + 
			geom_point() +
			scale_color_manual(values = c("salmon", "grey"), name = "Category", labels = c("Vandalism", "Thefts")) 
```

\newpage

* Part 2 - In accordance with question 2, I split the dataset into a 50% training and testing; and fit a kNN model. The function knnMR returns the misclassification rate for kNN given arguments K, a training dataset, and a validation dataset. 

```{r question2_part2, include = TRUE, echo = TRUE, cache = TRUE, results = "hide"}
data <- data[ ,.(Category, X, Y)]
data[ , ':=' (
	Category = as.factor(Category),
	X 		 = as.numeric(X),
	Y 		 = as.numeric(Y)
)]

idx 	 <- sample(1:NROW(data), NROW(data) * 0.50) # 50% random split
train 	 <- data[ idx, ]
validate <- data[-idx, ]

knnMR <- function(K, train, validate){ 
# does KNN for selected K and reports missclassification rate
knn 	<- kknn(formula = Category ~ X + Y, train = train, 
				test = validate, kernel = "rectangular", k = K)
yhat 	<- as.numeric(knn$fitted.values) - 1 # turn into binary; Vandalism = 1, Theft = 0
true_y  <- as.numeric(validate$Category) - 1
MR 		<- lossMR(true_y, yhat)
return(MR)
}

result <- as.vector(NULL)
for(k in 1:100) result[k] <- knnMR(k, train, validate)
			
```

\newpage

+ A) A scatterplot of the misclassification rate on the validation set against the parameter k:

```{r question2_part2a, include = TRUE, echo = FALSE, height = 20}

ggplot(data = data.frame(result), aes(x = 1:NROW(result), y = result)) + 
		xlab("K") + ylab("Misclassification Rate") +
		geom_line(color = 'grey') + 
		geom_point()

```

+ B) The optimal k selected by the validation-set approach and the minimum misclassification rate on the
validation set:

```{r question2_part2b, include = TRUE, echo=TRUE, cache = FALSE}

which.min(result) # optimal k

result[which.min(result)] # min MR

```

\newpage

+ C) A single scatterplot of the crime incidents using their latitudes (X) and longitudes (Y)

```{r question2_part2c, include = TRUE, echo=TRUE, cache = FALSE}

plotBestKNN <- function(x, train, validate){

knn 	<- kknn(formula = Category ~ X + Y, train = train, 
              test = validate, kernel = "rectangular", k = which.min(x)) # at best K
DT 		<- cbind(validate, pred_Category = knn$fitted.values) # add the predicted column

p <- ggplot(data = DT, aes(x = X, y = Y, color = pred_Category)) + 
			geom_point() +
			scale_color_manual(values = c("salmon", "grey"), 
			                   name = "Predicted Category", 
			                   labels = c("Vandalism", "Thefts"))
return(p)				
}

plotBestKNN(x = result, train, validate)

```

\newpage

* Part 3 - In accordance with question 2, I repeat the part 2 20 times:

```{r question2_part3, include = TRUE, echo=TRUE, cache = TRUE}

registerDoMC(detectCores() - 1) # detect number of cores to split work apart
detectCores() # boothGrid is awesome @ 64 cores!

idxs 	 <- replicate(20, sample(1:NROW(data), NROW(data) * 0.50))
# 20, 50% random split; must create outside of foreach
results  <- foreach(i = 1:20) %dopar% {
# resample data 20 times and find optimal k on validation set using devils seed
idx 	 <- idxs[ ,i]					
train 	 <- data[ idx, ]
validate <- data[-idx, ]

result <- as.vector(NULL)
for(k in 1:100) result[k] <- knnMR(k, train, validate)
return(list(result = result, train = train, validate = validate))
}

```

+ A) The 20 optimal k’s selected by the validation sets and a scatterplot of the misclassification rate on the validation set against the parameter k.

```{r question2_part3a1, include = TRUE, echo = TRUE, cache = FALSE}
t(matrix(lapply(results, function(x) which.min(x$result)), 
          dimnames = list(1:length(results), "K"))) # best K at each N
```

```{r question2_part3a2, include = TRUE, echo = FALSE, cache = FALSE}

missclass <- as.data.frame(cbind(1:100, do.call(cbind, lapply(results, function(x) x$result))))
missclass_long <- melt(missclass, id = "V1")

ggplot(data = missclass_long, aes(x = V1, y = value, group = variable)) +
	geom_line(color = 'grey') +
	stat_summary(fun.y = mean, geom = "line", lwd = 1, aes(group = 1)) +
	xlab("K") + ylab("Misclassification Rate") 

```

+ B) The average of the minimum out-of-sample misclassification rates as well as its standard error.

```{r question2_part3b, include = TRUE, echo=TRUE, cache = FALSE}

mean(unlist(lapply(results, function(x) min(x$result)))) # mean of minimumn OOS classification rate
min.mr <- unlist(lapply(results, function(x) min(x$result)))
sd(min.mr) / sqrt(length(min.mr)) # SE

```

* Part 4 - In accordance with question 4, I repeat the part 2 20 times at 90/10 split:

```{r question2_part4, include = TRUE, echo = FALSE, cache = TRUE}

			idxs 	 <- replicate(20, sample(1:NROW(data), NROW(data) * 0.90)) # 20, 90% random split; must create outside of foreach
 			results  <- foreach(i = 1:20) %dopar% { # resample data 20 times and find optimal k on validation set using devils seed

 				idx 	 <- idxs[ ,i]					
				train 	 <- data[ idx, ]
				validate <- data[-idx, ]

				result <- as.vector(NULL)
				for(k in 1:100) result[k] <- knnMR(k, train, validate)
				return(list(result = result, train = train, validate = validate))
			}

```

+ A) A scatterplot of the misclassification rate on the validation set against the parameter k with the average misclassification rate in bold.

```{r question2_part4a, include = TRUE, echo = FALSE, cache = FALSE}
missclass <- as.data.frame(cbind(1:100, do.call(cbind, lapply(results, function(x) x$result))))
missclass_long <- melt(missclass, id = "V1")

ggplot(data = missclass_long, aes(x = V1, y = value, group = variable)) +
	geom_line(color = 'grey') +
	stat_summary(fun.y = mean, geom = "line", lwd = 1, aes(group = 1)) +
	xlab("K") + ylab("Misclassification Rate") 
```

+ B) A scatterplot of the crime incidents (Y vs X) with points colored according to predicted class given by the optimal k; I chose to show the scatterplot at n = 15:

```{r question2_part4b, include = TRUE, echo = TRUE, cache = FALSE}

n <- 15
plotBestKNN(x = results[[n]]$result, results[[n]]$train, results[[n]]$validate)

```

+ C) The 20 optimal k’s selected by the validation sets:

```{r question2_part4c, include = TRUE, echo = TRUE, cache = FALSE}

t(matrix(lapply(results, function(x) which.min(x$result)), 
         dimnames = list(1:length(results), "K"))) # best K at each N

```

+ D) Average minimum out-of-sample misclassification rates and its standard error.

```{r question2_part4d, include = TRUE, echo = TRUE, cache = FALSE}

mean(unlist(lapply(results, function(x) min(x$result)))) # average min OOS error
min.mr <- unlist(lapply(results, function(x) min(x$result)))
sd(min.mr) / sqrt(length(min.mr)) # SE

```

\newpage

* Part 5 - Comment on the difference between the results obtained in (2), (3) and (4).

Running a single kNN through k = 1:100 yields an optimal k of 10 at a misclassification rate of 32.2%; by chance, this single iteration predicts close to the mean value of 31.6% found in part 3. This is luck. We saw in the graph of 20 iterations of kNN that the variability of the optimal k and its corresponding misclassification rate ranges from 33% to 31%. It is only when I average (solid black line) the function of k on its misclassification rate do I get the most robust estimate of k. (k ~ 8). 

From this, we notice that the data prefer a more complex model; as k increases in size, the misclassification rate continuously decreases and even does worse than a single nearest neighbor prediction (k = 1). 

Naturally, as I increase the amount of training data available to kNN, 50% to 90%, the model predicts better. The mean OOS error falls from 31.6% to 29.7% and the S.E. decreases appropriately. This is expected. As I include more observations for kNN to train upon, the better the model performs.

* Part 6 - I split the data into 50% training/testing, use kNN @ k = 5, and plot the ROC curve:

```{r question2_part6, include = TRUE, echo = TRUE, cache = FALSE, results = 'hide'}

idx 	 <- sample(1:NROW(data), NROW(data) * 0.50) # 50% random split
train 	 <- data[ idx, ]
validate <- data[-idx, ]

knn 	<- kknn(formula = Category ~ X + Y, 
              train = train, 
             test = validate, 
             kernel = "rectangular", 
             k = 25) # at K = 25

yhat 	<- as.numeric(knn$fitted.values) - 1 # turn into binary; Vandalism = 1, Theft = 0
true_y  <- as.numeric(validate$Category) - 1

pROC:::roc(response = true_y, predictor = yhat, plot = TRUE) # ROC curve, AUC is 0.665

```

\newpage
## Question 3

Using the expected value framework, I would follow the following steps:

1. Predict whether a alumnus will donate or not (phat).
2. Determine the amount an alumnus will donate (yhat).
3. Derive a cost/benefit matrix to determine the minimum profitable probability threshold to target potential donors.
See Figure: Expected Value Framework

```{r question3_part2, include = TRUE, echo = TRUE, cache = FALSE}
# get probabiliy of donation from some model: LASSO, RF, GBM, SVM, DNN, etc
# phats <- predict(SOME_MODEL, newdata = x)
```

Next, I would use another model to find the benefit(yhat) from a donor. In this example, I use a fixed amount of $3000. Perhaps the donation amount may follow some probability distribution, ie log-normal 

```{r question3_part2.2, include = TRUE, echo = TRUE, cache = FALSE}
# get benefit given a donation of donation from some model: LASSO, RF, GBM, SVM, DNN, etc
# yhats <- predict(SOME_MODEL, newdata = x)
```

Then, I would derive a confusion matrix:

```{r question3_part2.3, include = TRUE, echo = TRUE, cache = FALSE}

# make example confusion matrix
# benefit of responding; this is from yhats ($3000 is an example)
benefit <- 3000 
cost <- 15 # fixed cost of targeting
matrix(c(benefit - cost, benefit, cost, 0), 
		nrow = 2,  
		dimnames = list(c("Target", "Not Target"),
                          c("Donate", "Not Donate"))
  ) 
```

Finally, using the probabilities from model 1 and the expected benefit from model 2, I would solve the following equation to find the minimum profitable threshold needed to target an individual. Any individual that is flagged higher than this phat would be targeted.

```{r question3_part2.4, include = TRUE, echo = TRUE, cache = FALSE}
# phat * benefit + (1 - phat) * cost > 0 # solve for phat; target those above phat
```

 ![Expected Value Framework](/home/nmatare/projects/machine_learning/midterm/costben.png)


\newpage

## Question 4

```{r question4, include = TRUE, echo = FALSE, cache = FALSE, results = 'hide'}

		data <- as.data.table(read.csv("Tayko.csv"))
		data[ , ':=' (

			US = as.logical(US),

			source_a 		 = as.logical(source_a),
			source_c 		 = as.logical(source_c),
			source_b 		 = as.logical(source_b),
			source_d 		 = as.logical(source_d),
			source_e 		 = as.logical(source_e),
			source_m 		 = as.logical(source_m),
			source_o 		 = as.logical(source_o),
			source_h 		 = as.logical(source_h),
			source_r 		 = as.logical(source_r),
			source_s 		 = as.logical(source_s),
			source_t 		 = as.logical(source_t),
			source_u 		 = as.logical(source_u),
			source_p 		 = as.logical(source_p),
			source_x 		 = as.logical(source_x),
			source_w 		 = as.logical(source_w),

			Freq 		 				 = as.numeric(Freq),
			last_update_days_ago 		 = as.numeric(last_update_days_ago),
			first_update_days_ago 		 = as.numeric(first_update_days_ago),
			Web_order 		 			 = as.logical(Web_order),
			Gender_is_male 		 		 = as.logical(Gender_is_male),
			Address_is_res 		 		 = as.logical(Address_is_res),
			Purchase 		 			 = as.logical(Purchase),
			Spending 		 			 = as.numeric(Spending),
			Partition 		 			 = as.factor(Partition)
		)]

```

* Part 1 - Gross Profit:

```{r question4_part1, include = TRUE, echo = TRUE, cache = FALSE}

total.spend <- sum(data[Partition == 's' ,Spending])
num.customer <- length(data[Partition == 's' ,Spending])

(total.spend / num.customer * 0.107 - 2) * 18e4 / 1e6 # gross profit in millions

```

* Part 2 - I begin by combining the training and validation partitions and removing erroneous columns. I combine the training and validation partitions because I do cross validation automatically in the subsequent packages.

```{r question4_part2_0, include = TRUE, echo = TRUE, cache = FALSE}

# combine training and validation together; remove spending
train_p <- data[Partition %in% c('t', 'v'), 
                !which(colnames(data) %in% 
                c('Spending', 'sequence_number', 'Partition')), 
                with = FALSE]

test_p <- data[Partition %in% c('s'), 
               !which(colnames(data) %in% 
               c('Spending', 'sequence_number', 'Partition')), 
               with = FALSE] 


```

First I try a linear model:

```{r question4_part2_1, include = TRUE, echo = TRUE, cache = FALSE}

LASSO <- cv.gamlr(	x = train_p[ ,which(colnames(train_p) != 'Purchase'), with = FALSE], 
					y = train_p[ ,Purchase], 
					family = 'binomial', verb = FALSE, lambda.start = 0.1, nfold = 10)

yhat <- predict(LASSO, newdata = train_p[ ,which(colnames(train_p) != 'Purchase'), 
                                          with = FALSE], type = 'response')

lossMR(train_p[ ,Purchase], yhat) # LASSO prediction error

```

Second, I try a random forest:

```{r question4_part2_2, include = TRUE, echo = TRUE, cache = TRUE}

RF <- ranger(	as.factor(Purchase) ~., 	data = train_p, 
				probability = TRUE, classification = TRUE, num.trees = 5000, 
				write.forest = TRUE, num.threads = detectCores() - 1, 
				importance = 'impurity', verbose = TRUE
)

RF$prediction.error * 100 # OOB prediction error

```

Finally, I employ boosted trees:

```{r question4_part2_3, include = TRUE, echo = TRUE, cache = TRUE, results = 'hide'}

params <- list(max_depth = 4, booster = "gbtree", objective = "binary:logistic") 
XGBST.cv <- xgb.cv(	params = params, 
					data = as.matrix(train_p[ ,which(colnames(train_p) != 'Purchase'), with = FALSE]), 
					label = as.vector(train_p[ ,Purchase]),
                   		nthread = detectCores() - 1, verbose = 1, nfold = 10, nrounds = 100)
```
```{r question4_part2_31, include = TRUE, echo = TRUE, cache = FALSE}

tail(XGBST.cv$evaluation$test_error_mean, 1) # last cv.fold missclassification error

```

Thus, I chose bagged trees (Random Forest) because it fits the data the best -- it has the lowest misclassification rate at 12.2%

* Part 3 - A ROC curve for the chosen model in the previous step on the test data.

```{r question4_part3, include = TRUE, echo = TRUE, cache = FALSE, results = 'hide'}

phat <- ranger:::predict.ranger(RF, 
                                test_p[ ,which(colnames(test_p) != 'Purchase'), 
                                            with = FALSE])$predictions[ ,'TRUE']

# ROC curve, AUC is 0.923
pROC:::roc(response = test_p[ ,Purchase], predictor = phat, plot = TRUE) 
		
```

\newpage

* Part 4 - I again combine the training and validation partitions and remove erroneous columns. I combine the training and validation partitions because I do cross validation automatically in the subsequent packages. I only include the observations where users purchased a catalog.

```{r question4_part4_0, include = TRUE, echo = TRUE, cache = FALSE}

# combine training and validation together; remove spending
train_s <- data[Partition %in% c('t', 'v') & Purchase == TRUE, 
                !which(colnames(data) %in% 
                c('sequence_number', 'Partition', 'Purchase')), with = FALSE]

test_s <- data[Partition %in% c('s'), # notice don't know purchase outcome, so it is not subset
               !which(colnames(data) %in% 
               c('sequence_number', 'Partition', 'Purchase')), with = FALSE] 

```

First I try a linear model:

```{r question4_part4_1, include = TRUE, echo = TRUE, cache = FALSE}

LASSO <- cv.gamlr(	x = train_s[ ,which(colnames(train_s) != 'Spending'), with = FALSE], 
					y = train_s[ ,Spending], 
					family = 'gaussian', verb = FALSE, lambda.start = Inf, nfold = 10)

sqrt(LASSO$cvm[which.min(LASSO$cvm)]) # RMSE

```

Second, I try a random forest:

```{r question4_part4_2, include = TRUE, echo = TRUE, cache = TRUE}

RF <- ranger(	Spending ~., 	data = train_s, 
				probability = FALSE, classification = FALSE, num.trees = 5000, 
				write.forest = TRUE, num.threads = detectCores() - 1, 
				importance = 'impurity', verbose = TRUE
)

sqrt(RF$prediction.error) # RMSE

```

Finally, I employ boosted trees:

```{r question4_part4_3, include = TRUE, echo = TRUE, cache = TRUE, results = 'hide'}

params <- list(max_depth = 4, booster = "gbtree", objective = "reg:linear") 
XGBST.cv <- xgb.cv(	params = params, 
					data = as.matrix(train_s[ ,which(colnames(train_s) != 'Spending'), with = FALSE]), 
					label = as.vector(train_s[ ,Spending]),
          nthread = detectCores() - 1, verbose = 1, nfold = 10, nrounds = 100)
```

```{r question4_part4_31, include = TRUE, echo = TRUE, cache = FALSE}
tail(XGBST.cv$evaluation$test_rmse_mean, 1) # last cv.fold missclassification error; best RMSE
```

Thus, I chose boosted trees because they fit the data the best -- the model has the lowest RMSE at 167.

\newpage

* Part 5 - Using the best parameters found in cross validation, I fit boosted trees to the training/validation data and predict on the testing partition:

```{r question4_part5_0, include = TRUE, echo = TRUE, cache = FALSE, results = 'hide'}

XGBST <- xgboost(	params = params, 
                  data = as.matrix(train_s[ ,which(colnames(train_s) != 'Spending'), with = FALSE]), 
					        label = as.vector(train_s[ ,Spending]), 
					        nthread = detectCores() - 1, verbose = 1, nrounds = 500)

# predicted spending given best model
yhat <- predict(XGBST, as.matrix(test_s[ ,which(colnames(test_s) != 'Spending'), with = FALSE]))
yhat <- ifelse(yhat <= 0, 0, yhat) # not possible for neg yhats

```

Next, I place the probability or responding into the testing subset and record the predicted spending for those who purchased:

```{r question4_part5_1, include = TRUE, echo = TRUE, cache = FALSE, results = 'hide'}

# predicted spending given best model
yhat <- predict(XGBST, as.matrix(test_s[ ,which(colnames(test_s) != 'Spending'), with = FALSE])) 
yhat <- ifelse(yhat <= 0, 0, yhat) # not possible for neg yhats

test_p[ ,phat := phat] # phats to purchase data.table
test_p[ ,yhat := yhat] # yhats to those that purchased
total.spend <- sum(data[Partition == 's' ,Spending])
num.customer <- length(data[Partition == 's' ,Spending])

```

Finally, I plot the expected spending:

```{r question4_part5_2, include = TRUE, echo = TRUE, cache = FALSE, results = 'hide'}

expected_spending <- test_p[ ,cumsum(sort(yhat * phat * 0.107)) / (total.spend / num.customer)]
ggplot(data = data.frame(expected_spending), 
       aes(x = 1:NROW(expected_spending), y = expected_spending)) + 
      geom_line() #xlab("K") + ylab("Misclassification Rate") 

```

* Bonus - First I calculate the marginal revenue given from Q4.1. Second I get the percentile score of the top 3.6% of the remaining list; finally I calculate the new gross-profit. Naturally, targeting customers (instead of randomly targeting them) yields a higher profit of $7.56M.

```{r question4_part5_bonus, include = TRUE, echo = TRUE, cache = FALSE}

mr <- (total.spend / num.customer * 0.107 - 2) # marginal revenue from Q4.1
n <- 500 * 18e4 / 5e6 # mail the top 3.6% of the list 
lift <- test_p[ ,cumsum(sort(phat * yhat))][n] # lift at n percentile
(lift * (total.spend / num.customer * 0.107) - 2) * 18e4 / 1e6 # new gross profit

```